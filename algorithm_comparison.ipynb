{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Reinforcement Learning: Algorithm Comparison\n",
        "\n",
        "This notebook demonstrates and compares four actor-critic algorithms for continuous control:\n",
        "- **DDPG** - Deep Deterministic Policy Gradient (original)\n",
        "- **OurDDPG** - Re-tuned DDPG with 256-256 architecture\n",
        "- **TD3** - Twin Delayed Deep Deterministic Policy Gradient\n",
        "- **QRTD3** - Quantile Regression TD3\n",
        "\n",
        "All implementations are imported from the Python files in this repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the environment and seed here. You can also choose which algorithm to run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration:\n",
            "  Environment: Reacher-v5\n",
            "  Seed: 0\n",
            "  Policy: TD3\n",
            "  Max timesteps: 1,000,000\n"
          ]
        }
      ],
      "source": [
        "# Configuration - Set these at the beginning\n",
        "ENV_NAME = \"Reacher-v5\"  # Options: Reacher-v5, Ant-v5, HalfCheetah-v5, Hopper-v5, Walker2d-v5, etc.\n",
        "SEED = 0\n",
        "POLICY = \"TD3\"  # Options: \"TD3\", \"DDPG\", \"OurDDPG\", \"QRTD3\"\n",
        "\n",
        "# Training hyperparameters\n",
        "MAX_TIMESTEPS = int(1e6)  # 1M timesteps\n",
        "START_TIMESTEPS = int(25e3)  # Initial random exploration\n",
        "EVAL_FREQ = int(5e3)  # Evaluation frequency\n",
        "BATCH_SIZE = 256\n",
        "DISCOUNT = 0.99\n",
        "TAU = 0.005\n",
        "POLICY_NOISE = 0.2\n",
        "NOISE_CLIP = 0.5\n",
        "POLICY_FREQ = 2\n",
        "EXPL_NOISE = 0.1\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Environment: {ENV_NAME}\")\n",
        "print(f\"  Seed: {SEED}\")\n",
        "print(f\"  Policy: {POLICY}\")\n",
        "print(f\"  Max timesteps: {MAX_TIMESTEPS:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "\n",
        "Import the necessary libraries and algorithm implementations from the Python files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "PyTorch version: 2.9.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import algorithm implementations\n",
        "import utils\n",
        "import TD3\n",
        "import DDPG\n",
        "import OurDDPG\n",
        "import QR_TD3\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm Overview\n",
        "\n",
        "### DDPG (Deep Deterministic Policy Gradient)\n",
        "- **Paper**: [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971) (Lillicrap et al., 2015)\n",
        "- Single critic network (400-300 architecture)\n",
        "- Actor network (400-300 architecture)\n",
        "- Updates policy every step\n",
        "- Learning rate: 1e-4 (actor), weight_decay=1e-2 (critic)\n",
        "- Tau: 0.001\n",
        "\n",
        "### OurDDPG (Re-tuned DDPG)\n",
        "- Same as DDPG but with 256-256 architecture\n",
        "- Learning rate: 3e-4\n",
        "- Tau: 0.005\n",
        "- Batch size: 256\n",
        "\n",
        "### TD3 (Twin Delayed Deep Deterministic Policy Gradient)\n",
        "- **Paper**: [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) (Fujimoto et al., 2018)\n",
        "- **Key improvements over DDPG**:\n",
        "  1. **Clipped Double Q-Learning**: Uses two critic networks, takes minimum to reduce overestimation\n",
        "  2. **Delayed Policy Updates**: Updates policy every 2 steps (less frequent than critics)\n",
        "  3. **Target Policy Smoothing**: Adds clipped noise to target actions\n",
        "- Twin critics (256-256 architecture each)\n",
        "- Actor network (256-256 architecture)\n",
        "- Learning rate: 3e-4\n",
        "- Tau: 0.005\n",
        "\n",
        "### QRTD3 (Quantile Regression TD3)\n",
        "- Extension of TD3 using quantile regression for the critic\n",
        "- Uses quantile loss instead of MSE loss\n",
        "- Can better capture uncertainty in value estimates\n",
        "- Same architecture as TD3 but with quantile-based critics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function\n",
        "\n",
        "This function evaluates the policy over multiple episodes without exploration noise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "    \"\"\"Evaluate the policy over multiple episodes.\"\"\"\n",
        "    eval_env = gym.make(env_name)\n",
        "    avg_reward = 0.\n",
        "    for _ in tqdm(range(eval_episodes), desc=\"Evaluating\", leave=False):\n",
        "        state = eval_env.reset(seed=seed + 100)[0]\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy.select_action(np.array(state))\n",
        "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "            done = terminated or truncated\n",
        "            avg_reward += reward\n",
        "    \n",
        "    avg_reward /= eval_episodes\n",
        "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "    return avg_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Setup\n",
        "\n",
        "Initialize the environment and policy based on the configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State dimension: 10\n",
            "Action dimension: 2\n",
            "Max action: 1.0\n",
            "\n",
            "Policy TD3 initialized successfully!\n",
            "Replay buffer initialized\n"
          ]
        }
      ],
      "source": [
        "# Create environment\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# Set seeds\n",
        "env.action_space.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Get environment dimensions\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "\n",
        "print(f\"State dimension: {state_dim}\")\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Max action: {max_action}\")\n",
        "\n",
        "# Initialize policy\n",
        "kwargs = {\n",
        "    \"state_dim\": state_dim,\n",
        "    \"action_dim\": action_dim,\n",
        "    \"max_action\": max_action,\n",
        "    \"discount\": DISCOUNT,\n",
        "    \"tau\": TAU,\n",
        "}\n",
        "\n",
        "if POLICY == \"TD3\":\n",
        "    kwargs[\"policy_noise\"] = POLICY_NOISE * max_action\n",
        "    kwargs[\"noise_clip\"] = NOISE_CLIP * max_action\n",
        "    kwargs[\"policy_freq\"] = POLICY_FREQ\n",
        "    policy = TD3.TD3(**kwargs)\n",
        "elif POLICY == \"QRTD3\":\n",
        "    kwargs[\"policy_noise\"] = POLICY_NOISE * max_action\n",
        "    kwargs[\"noise_clip\"] = NOISE_CLIP * max_action\n",
        "    kwargs[\"policy_freq\"] = POLICY_FREQ\n",
        "    policy = QR_TD3.QRTD3(**kwargs)\n",
        "elif POLICY == \"OurDDPG\":\n",
        "    policy = OurDDPG.DDPG(**kwargs)\n",
        "elif POLICY == \"DDPG\":\n",
        "    policy = DDPG.DDPG(**kwargs)\n",
        "else:\n",
        "    raise ValueError(f\"Unknown policy: {POLICY}\")\n",
        "\n",
        "print(f\"\\nPolicy {POLICY} initialized successfully!\")\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = utils.ReplayBuffer(state_dim, action_dim)\n",
        "print(f\"Replay buffer initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "The training loop follows the standard off-policy actor-critic procedure:\n",
        "1. Collect experience by interacting with the environment\n",
        "2. Store transitions in the replay buffer\n",
        "3. Sample batches and update the policy\n",
        "4. Periodically evaluate the policy without exploration noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating untrained policy...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                  \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n",
            "\n",
            "Starting training for 1,000,000 timesteps...\n",
            "Random exploration for first 25,000 steps\n",
            "Evaluation frequency: every 5,000 steps\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 6802/1000000 [00:00<01:42, 9720.69it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   1%|          | 11350/1000000 [00:01<01:43, 9563.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|▏         | 17038/1000000 [00:01<01:42, 9593.44it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|▏         | 21540/1000000 [00:02<01:43, 9488.82it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   2%|▏         | 24991/1000000 [00:02<01:30, 10744.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -5.469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   3%|▎         | 30029/1000000 [00:33<1:46:02, 152.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|▎         | 35017/1000000 [00:59<1:45:08, 152.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|▍         | 40030/1000000 [01:26<1:28:21, 181.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.970\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|▍         | 45028/1000000 [01:52<1:30:21, 176.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -3.703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   5%|▌         | 50031/1000000 [02:22<1:32:21, 171.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -1.222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|▌         | 55033/1000000 [02:48<1:28:17, 178.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -1.011\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   6%|▌         | 60018/1000000 [03:16<1:35:15, 164.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|▋         | 65016/1000000 [03:51<1:45:33, 147.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   7%|▋         | 70015/1000000 [04:20<1:35:48, 161.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|▊         | 75037/1000000 [04:48<1:28:51, 173.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   8%|▊         | 80040/1000000 [05:13<1:27:44, 174.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   9%|▊         | 85013/1000000 [05:41<3:38:58, 69.64it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   9%|▉         | 90020/1000000 [06:09<1:41:35, 149.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -1.041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|▉         | 95034/1000000 [06:38<1:26:41, 173.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  10%|█         | 100021/1000000 [07:10<2:31:08, 99.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.954\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|█         | 105011/1000000 [07:39<1:30:48, 164.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  11%|█         | 110026/1000000 [08:05<1:24:04, 176.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|█▏        | 115028/1000000 [08:31<1:22:18, 179.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  12%|█▏        | 120024/1000000 [08:56<1:22:11, 178.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|█▎        | 125018/1000000 [09:23<1:26:12, 169.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  13%|█▎        | 130024/1000000 [09:50<1:36:38, 150.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|█▎        | 135022/1000000 [10:20<1:38:21, 146.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  14%|█▍        | 140024/1000000 [10:50<1:26:29, 165.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|█▍        | 145028/1000000 [11:14<1:21:23, 175.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  15%|█▌        | 150038/1000000 [11:49<1:19:13, 178.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|█▌        | 155039/1000000 [12:25<1:23:06, 169.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.920\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  16%|█▌        | 160018/1000000 [12:56<1:33:47, 149.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  17%|█▋        | 165016/1000000 [13:35<2:12:28, 105.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  17%|█▋        | 170028/1000000 [14:14<1:11:59, 192.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  18%|█▊        | 175019/1000000 [14:48<1:58:44, 115.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  18%|█▊        | 180014/1000000 [15:25<2:06:54, 107.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  19%|█▊        | 185023/1000000 [15:59<1:40:54, 134.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  19%|█▉        | 190024/1000000 [16:36<1:42:27, 131.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|█▉        | 195018/1000000 [17:13<2:15:13, 99.21it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  20%|██        | 200031/1000000 [17:48<1:34:51, 140.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  21%|██        | 205009/1000000 [18:26<2:57:42, 74.56it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.939\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  21%|██        | 210028/1000000 [19:03<1:42:40, 128.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  22%|██▏       | 215016/1000000 [19:39<2:18:55, 94.17it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  22%|██▏       | 220027/1000000 [20:13<1:38:15, 132.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  23%|██▎       | 225016/1000000 [20:48<1:42:10, 126.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.890\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  23%|██▎       | 230018/1000000 [21:28<1:37:32, 131.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  24%|██▎       | 235016/1000000 [22:02<1:40:47, 126.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  24%|██▍       | 240026/1000000 [22:44<1:34:42, 133.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  25%|██▍       | 245028/1000000 [23:19<1:40:02, 125.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  25%|██▌       | 250017/1000000 [24:04<1:43:06, 121.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  26%|██▌       | 255017/1000000 [24:40<1:33:30, 132.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  26%|██▌       | 260023/1000000 [25:17<1:58:53, 103.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.949\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  27%|██▋       | 265021/1000000 [25:52<1:25:57, 142.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  27%|██▋       | 270029/1000000 [26:26<1:27:14, 139.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  28%|██▊       | 275026/1000000 [27:01<1:31:48, 131.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.982\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  28%|██▊       | 280016/1000000 [27:48<1:46:36, 112.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  29%|██▊       | 285022/1000000 [28:22<1:25:18, 139.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  29%|██▉       | 290022/1000000 [28:55<1:32:18, 128.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  30%|██▉       | 295016/1000000 [29:28<1:26:18, 136.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  30%|███       | 300024/1000000 [30:01<1:27:27, 133.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  31%|███       | 305022/1000000 [30:34<1:29:43, 129.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  31%|███       | 310030/1000000 [31:07<1:22:46, 138.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  32%|███▏      | 315028/1000000 [31:40<1:24:58, 134.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  32%|███▏      | 320018/1000000 [32:14<2:01:40, 93.14it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  33%|███▎      | 325026/1000000 [32:51<1:22:15, 136.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.946\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  33%|███▎      | 330024/1000000 [33:24<1:29:24, 124.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  34%|███▎      | 335023/1000000 [33:59<1:23:34, 132.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  34%|███▍      | 340027/1000000 [34:34<1:28:04, 124.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  35%|███▍      | 345013/1000000 [35:27<3:27:31, 52.60it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  35%|███▌      | 350015/1000000 [36:10<1:29:54, 120.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  36%|███▌      | 355027/1000000 [36:50<1:21:53, 131.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  36%|███▌      | 360020/1000000 [37:33<1:31:32, 116.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.915\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  37%|███▋      | 365009/1000000 [39:15<2:38:48, 66.64it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  37%|███▋      | 370019/1000000 [40:29<2:08:22, 81.79it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.924\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  38%|███▊      | 375017/1000000 [41:47<2:17:23, 75.82it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.940\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  38%|███▊      | 380016/1000000 [43:11<2:14:27, 76.85it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  39%|███▊      | 385016/1000000 [44:13<2:18:48, 73.84it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  39%|███▉      | 390016/1000000 [45:13<2:19:34, 72.84it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  40%|███▉      | 395008/1000000 [46:36<2:29:42, 67.35it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  40%|████      | 400006/1000000 [48:13<3:42:14, 45.00it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  41%|████      | 405014/1000000 [49:21<2:23:32, 69.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  41%|████      | 410012/1000000 [50:38<1:51:33, 88.14it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  42%|████▏     | 415018/1000000 [51:31<1:55:58, 84.07it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation over 10 episodes: -0.848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  42%|████▏     | 419073/1000000 [52:16<5:32:34, 29.11it/s] "
          ]
        }
      ],
      "source": [
        "# Evaluate untrained policy\n",
        "print(\"Evaluating untrained policy...\")\n",
        "evaluations = [eval_policy(policy, ENV_NAME, SEED)]\n",
        "\n",
        "# Initialize training variables\n",
        "state = env.reset(seed=SEED + 100)[0]\n",
        "done = False\n",
        "episode_reward = 0\n",
        "episode_timesteps = 0\n",
        "episode_num = 0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(f\"\\nStarting training for {MAX_TIMESTEPS:,} timesteps...\")\n",
        "print(f\"Random exploration for first {START_TIMESTEPS:,} steps\")\n",
        "print(f\"Evaluation frequency: every {EVAL_FREQ:,} steps\\n\")\n",
        "\n",
        "# Main training loop\n",
        "for t in tqdm(range(int(MAX_TIMESTEPS)), desc=\"Training\"):\n",
        "    episode_timesteps += 1\n",
        "    \n",
        "    # Select action randomly or according to policy with exploration noise\n",
        "    if t < START_TIMESTEPS:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = (\n",
        "            policy.select_action(np.array(state))\n",
        "            + np.random.normal(0, max_action * EXPL_NOISE, size=action_dim)\n",
        "        ).clip(-max_action, max_action)\n",
        "    \n",
        "    # Perform action in environment\n",
        "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "    \n",
        "    # Store transition in replay buffer\n",
        "    replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "    \n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "    \n",
        "    # Train agent after collecting sufficient data\n",
        "    if t >= START_TIMESTEPS:\n",
        "        policy.train(replay_buffer, BATCH_SIZE)\n",
        "    \n",
        "    if done:\n",
        "        # Reset environment\n",
        "        state = env.reset(seed=SEED + 100)[0]\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "    \n",
        "    # Evaluate episode\n",
        "    if (t + 1) % EVAL_FREQ == 0:\n",
        "        eval_reward = eval_policy(policy, ENV_NAME, SEED)\n",
        "        evaluations.append(eval_reward)\n",
        "\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "hours = int(duration // 3600)\n",
        "minutes = int((duration % 3600) // 60)\n",
        "seconds = int(duration % 60)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Training time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
        "print(f\"Final evaluation reward: {evaluations[-1]:.3f}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Visualization\n",
        "\n",
        "Plot the learning curve showing how the average reward improves over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot learning curve\n",
        "timesteps = np.array([0 if i == 0 else i * EVAL_FREQ for i in range(len(evaluations))])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(timesteps, evaluations, marker='o', linewidth=2, markersize=4, label=POLICY)\n",
        "plt.xlabel('Timestep', fontsize=14)\n",
        "plt.ylabel('Average Reward', fontsize=14)\n",
        "plt.title(f'{POLICY} Learning Curve on {ENV_NAME} (Seed {SEED})', fontsize=16, fontweight='bold')\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPerformance Summary:\")\n",
        "print(f\"  Initial reward: {evaluations[0]:.3f}\")\n",
        "print(f\"  Final reward: {evaluations[-1]:.3f}\")\n",
        "print(f\"  Improvement: {evaluations[-1] - evaluations[0]:.3f}\")\n",
        "print(f\"  Total evaluations: {len(evaluations)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated how to:\n",
        "1. Import algorithm implementations from Python files\n",
        "2. Configure environment and hyperparameters\n",
        "3. Train an agent using the off-policy actor-critic framework\n",
        "4. Visualize learning curves\n",
        "\n",
        "To compare different algorithms, simply change the `POLICY` variable at the top and re-run the training cells. All algorithms share the same interface (`select_action`, `train`), making it easy to compare them.\n",
        "\n",
        "### Key Differences Between Algorithms\n",
        "\n",
        "| Algorithm | Critic Networks | Policy Updates | Special Features |\n",
        "|-----------|----------------|----------------|------------------|\n",
        "| DDPG | Single (400-300) | Every step | - |\n",
        "| OurDDPG | Single (256-256) | Every step | Re-tuned hyperparameters |\n",
        "| TD3 | Twin (256-256) | Every 2 steps | Clipped Double Q, Target Smoothing |\n",
        "| QRTD3 | Twin Quantile (256-256) | Every 2 steps | Quantile Regression |\n",
        "\n",
        "For more details, see the individual algorithm files: `DDPG.py`, `OurDDPG.py`, `TD3.py`, `QR_TD3.py`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
