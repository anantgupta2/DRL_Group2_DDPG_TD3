{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c14f8b6",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90219bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "import argparse\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import utils\n",
    "import TD3\n",
    "import DDPG\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7724f5",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a109021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "\t\tself.max_size = max_size\n",
    "\t\tself.ptr = 0\n",
    "\t\tself.size = 0\n",
    "\n",
    "\t\tself.state = np.zeros((max_size, state_dim))\n",
    "\t\tself.action = np.zeros((max_size, action_dim))\n",
    "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
    "\t\tself.reward = np.zeros((max_size, 1))\n",
    "\t\tself.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\tdef add(self, state, action, next_state, reward, done):\n",
    "\t\tself.state[self.ptr] = state\n",
    "\t\tself.action[self.ptr] = action\n",
    "\t\tself.next_state[self.ptr] = next_state\n",
    "\t\tself.reward[self.ptr] = reward\n",
    "\t\tself.not_done[self.ptr] = 1. - done\n",
    "\n",
    "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
    "\t\tself.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "\tdef sample(self, batch_size):\n",
    "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6274486",
   "metadata": {},
   "source": [
    "### Actor Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17ec41",
   "metadata": {},
   "source": [
    "#### Actor DDPG & Actor TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf65bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorDDPG(nn.Module):\n",
    "    \"\"\"Actor network for DDPG (from DDPG.py - 400-300 architecture).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(ActorDDPG, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))\n",
    "\n",
    "\n",
    "class ActorTD3(nn.Module):\n",
    "    \"\"\"Actor network for TD3 (from TD3.py - 256-256 architecture).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(ActorTD3, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, action_dim)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * torch.tanh(self.l3(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ea675",
   "metadata": {},
   "source": [
    "### Critic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b0f9d",
   "metadata": {},
   "source": [
    "#### Critic DDPG & Critic TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72032d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticDDPG(nn.Module):\n",
    "    \"\"\"Critic network for DDPG (from DDPG.py).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticDDPG, self).__init__()\n",
    "\n",
    "        # DDPG architecture: state first, then concat with action\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400 + action_dim, 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Process state first, then concatenate with action\n",
    "        q = F.relu(self.l1(state))\n",
    "        q = F.relu(self.l2(torch.cat([q, action], 1)))\n",
    "        return self.l3(q)\n",
    "\n",
    "\n",
    "class CriticTD3(nn.Module):\n",
    "    \"\"\"Twin critic networks for TD3 (from TD3.py).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticTD3, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.l5 = nn.Linear(256, 256)\n",
    "        self.l6 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Forward pass through both Q-networks.\"\"\"\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "\n",
    "        q2 = F.relu(self.l4(sa))\n",
    "        q2 = F.relu(self.l5(q2))\n",
    "        q2 = self.l6(q2)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        \"\"\"Get Q1 value only (used for actor update).\"\"\"\n",
    "        sa = torch.cat([state, action], 1)\n",
    "\n",
    "        q1 = F.relu(self.l1(sa))\n",
    "        q1 = F.relu(self.l2(q1))\n",
    "        q1 = self.l3(q1)\n",
    "        return q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa30755",
   "metadata": {},
   "source": [
    "### DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52e889af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDPG agent class defined (from DDPG.py)\n"
     ]
    }
   ],
   "source": [
    "class DDPG(object):\n",
    "    \"\"\"Deep Deterministic Policy Gradient (from DDPG.py - original paper implementation).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, max_action, discount=0.99, tau=0.001):\n",
    "        # Initialize actor and target actor (using ActorDDPG with 400-300 architecture)\n",
    "        self.actor = ActorDDPG(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)  # lr=1e-4\n",
    "\n",
    "        # Initialize critic and target critic\n",
    "        self.critic = CriticDDPG(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), weight_decay=1e-2)  # weight_decay=1e-2\n",
    "\n",
    "        self.discount = discount\n",
    "        self.tau = tau  # tau = 0.001\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using current policy (no exploration noise).\"\"\"\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=64):  # batch_size=64 (not 256)\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        # Sample replay buffer\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Compute the target Q value\n",
    "        target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_Q = reward + (not_done * self.discount * target_Q).detach()\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        \n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Load model parameters.\"\"\"\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "print(\"DDPG agent class defined (from DDPG.py)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77f25d",
   "metadata": {},
   "source": [
    "### TD3 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa59ed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD3 agent class defined (from TD3.py)\n"
     ]
    }
   ],
   "source": [
    "class TD3(object):\n",
    "    \"\"\"Twin Delayed Deep Deterministic Policy Gradient (from TD3.py).\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        max_action,\n",
    "        discount=0.99,\n",
    "        tau=0.005,\n",
    "        policy_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_freq=2\n",
    "    ):\n",
    "        # Initialize actor and target actor (using ActorTD3 with 256-256 architecture)\n",
    "        self.actor = ActorTD3(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "\n",
    "        # Initialize twin critics and target critics\n",
    "        self.critic = CriticTD3(state_dim, action_dim).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using current policy (no exploration noise).\"\"\"\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        \"\"\"Perform one training step.\"\"\"\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay buffer\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Select action according to policy and add clipped noise (Target Policy Smoothing)\n",
    "            noise = (\n",
    "                torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "            \n",
    "            next_action = (\n",
    "                self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute target Q value using minimum of two Q-networks (Clipped Double Q-Learning)\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Get current Q estimates from both critics\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss (MSE for both Q-networks)\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = None\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor loss (maximize Q1)\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "            \n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models using soft (Polyak) updates\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item() if actor_loss is not None else None\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"Load model parameters.\"\"\"\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "print(\"TD3 agent class defined (from TD3.py)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a583c6aa",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54c3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
    "    \"\"\"Evaluate the policy over multiple episodes.\"\"\"\n",
    "    eval_env = gym.make(env_name)\n",
    "    avg_reward = 0.\n",
    "    for _ in tqdm(range(eval_episodes), desc=\"Evaluating\", leave=False):\n",
    "        state = eval_env.reset(seed=seed + 100)[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.select_action(np.array(state))\n",
    "            state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            avg_reward += reward\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "\n",
    "    print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
    "    return avg_reward\n",
    "\n",
    "print(\"Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9129c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Reacher-v5\n",
      "Seed: 0\n",
      "Max timesteps: 200,000\n",
      "Start timesteps: 10,000\n",
      "Evaluation frequency: 5,000\n",
      "\n",
      "Note: DDPG uses tau=0.001, TD3 uses tau=0.005\n",
      "Note: Reduced timesteps for Reacher-v5 (typically converges in <200K steps)\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "env_name = \"Reacher-v5\"  # Fastest environment for training\n",
    "seed = 0\n",
    "\n",
    "# Reduced timesteps for faster convergence analysis\n",
    "# Reacher typically converges much faster than 1M steps\n",
    "max_timesteps = int(2e5)  # Reduced from 1M to 200K for faster experimentation\n",
    "start_timesteps = int(1e4)  # Reduced from 25K to 10K\n",
    "eval_freq = int(5e3)  # Evaluate every 5K steps\n",
    "\n",
    "batch_size = 256\n",
    "discount = 0.99\n",
    "tau = 0.005  # Default tau (used by TD3)\n",
    "# Note: DDPG uses tau=0.001 (different from TD3's tau=0.005)\n",
    "\n",
    "# TD3-specific parameters\n",
    "policy_noise = 0.2\n",
    "noise_clip = 0.5\n",
    "policy_freq = 2  # Delayed policy update frequency\n",
    "\n",
    "expl_noise = 0.1  # Exploration noise std\n",
    "save_model = True  # Whether to save models\n",
    "\n",
    "print(f\"Environment: {env_name}\")\n",
    "print(f\"Seed: {seed}\")\n",
    "print(f\"Max timesteps: {max_timesteps:,}\")\n",
    "print(f\"Start timesteps: {start_timesteps:,}\")\n",
    "print(f\"Evaluation frequency: {eval_freq:,}\")\n",
    "print(f\"\\nNote: DDPG uses tau=0.001, TD3 uses tau=0.005\")\n",
    "print(f\"Note: Reduced timesteps for Reacher-v5 (typically converges in <200K steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de169f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State dimension: 10\n",
      "Action dimension: 2\n",
      "Max action: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Set seeds\n",
    "env.action_space.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Get environment dimensions\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "print(f\"Max action: {max_action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb89d619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training DDPG\n",
      "======================================================================\n",
      "DDPG agent initialized\n",
      "Architecture: 400-300 hidden layers (from DDPG.py)\n",
      "Replay buffer capacity: 1,000,000 transitions\n",
      "\n",
      "Evaluating untrained DDPG policy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation over 10 episodes: -19.542\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ddpg_writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating untrained DDPG policy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m ddpg_evaluations \u001b[38;5;241m=\u001b[39m [eval_policy(ddpg_policy, env_name, seed)]\n\u001b[0;32m---> 25\u001b[0m \u001b[43mddpg_writer\u001b[49m\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluation/Average_Reward\u001b[39m\u001b[38;5;124m'\u001b[39m, ddpg_evaluations[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize training variables\u001b[39;00m\n\u001b[1;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mseed)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ddpg_writer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Training DDPG\")\n",
    "print(\"=\"*70)\n",
    "# Initialize DDPG agent with tau=0.001 (from DDPG.py)\n",
    "ddpg_policy = DDPG(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    max_action=max_action,\n",
    "    discount=discount,\n",
    "    tau=0.001  # DDPG.py uses 0.001, not 0.005\n",
    ")\n",
    "\n",
    "# Initialize replay buffer\n",
    "ddpg_replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "\n",
    "print(\"DDPG agent initialized\")\n",
    "print(f\"Architecture: 400-300 hidden layers (from DDPG.py)\")\n",
    "print(f\"Replay buffer capacity: 1,000,000 transitions\")\n",
    "\n",
    "# Evaluate untrained policy\n",
    "print(\"\\nEvaluating untrained DDPG policy...\")\n",
    "ddpg_evaluations = [eval_policy(ddpg_policy, env_name, seed)]\n",
    "ddpg_writer.add_scalar('Evaluation/Average_Reward', ddpg_evaluations[0], 0)\n",
    "\n",
    "# Initialize training variables\n",
    "state = env.reset(seed=seed)[0]\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "ddpg_start_time = time.time()\n",
    "\n",
    "print(f\"\\nStarting DDPG training for {max_timesteps:,} timesteps...\")\n",
    "print(f\"Random exploration for first {start_timesteps:,} steps\")\n",
    "print(f\"Evaluation frequency: every {eval_freq:,} steps\")\n",
    "print(f\"Watch TensorBoard above for real-time updates!\\n\")\n",
    "\n",
    "# Main training loop\n",
    "for t in tqdm(range(int(max_timesteps)), desc=\"Training DDPG\"):\n",
    "    \n",
    "    episode_timesteps += 1\n",
    "\n",
    "    # Select action randomly or according to policy with exploration noise\n",
    "    if t < start_timesteps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = (\n",
    "            ddpg_policy.select_action(np.array(state))\n",
    "            + np.random.normal(0, max_action * expl_noise, size=action_dim)\n",
    "        ).clip(-max_action, max_action)\n",
    "\n",
    "    # Perform action in environment\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
    "\n",
    "    # Store transition in replay buffer\n",
    "    ddpg_replay_buffer.add(state, action, next_state, reward, done_bool)\n",
    "\n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Train agent after collecting sufficient data\n",
    "    if t >= start_timesteps:\n",
    "        critic_loss, actor_loss = ddpg_policy.train(ddpg_replay_buffer, batch_size)\n",
    "        \n",
    "        # Log losses to TensorBoard\n",
    "        if t % 1000 == 0:\n",
    "            ddpg_writer.add_scalar('Loss/Critic', critic_loss, t)\n",
    "            ddpg_writer.add_scalar('Loss/Actor', actor_loss, t)\n",
    "\n",
    "    if done:\n",
    "        # Log episode reward\n",
    "        ddpg_writer.add_scalar('Training/Episode_Reward', episode_reward, episode_num)\n",
    "        ddpg_writer.add_scalar('Training/Episode_Length', episode_timesteps, episode_num)\n",
    "        \n",
    "        # Reset environment\n",
    "        state = env.reset(seed=seed)[0]\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        episode_timesteps = 0\n",
    "        episode_num += 1\n",
    "\n",
    "    # Evaluate episode\n",
    "    if (t + 1) % eval_freq == 0:\n",
    "        eval_reward = eval_policy(ddpg_policy, env_name, seed)\n",
    "        ddpg_evaluations.append(eval_reward)\n",
    "        ddpg_writer.add_scalar('Evaluation/Average_Reward', eval_reward, t + 1)\n",
    "        \n",
    "        if save_model:\n",
    "            os.makedirs(f\"./notebook_models/{env_name}\", exist_ok=True)\n",
    "            ddpg_policy.save(f\"./notebook_models/{env_name}/DDPG_seed{seed}\")\n",
    "\n",
    "ddpg_end_time = time.time()\n",
    "ddpg_duration = ddpg_end_time - ddpg_start_time\n",
    "\n",
    "print(\"\\nDDPG Training completed!\")\n",
    "hours = int(ddpg_duration // 3600)\n",
    "minutes = int((ddpg_duration % 3600) // 60)\n",
    "seconds = int(ddpg_duration % 60)\n",
    "print(f\"Training time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "print(f\"Final evaluation reward: {ddpg_evaluations[-1]:.3f}\")\n",
    "\n",
    "ddpg_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
